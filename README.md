# Housing-Prices-Project-Competition-for-Kaggle-Learn-Users
</br>

## 1. Project Title
House Prices - Advanced Regression Techniques

## 2. Problem Statement or Research Question
</br>
‚ùì Research Questions :</br>
1. Can more refined feature engineering and model tuning improve prediction accuracy and achieve a higher Kaggle ranking?</br>
2. How can we build a machine learning model to accurately predict house prices?</br>
3. What are the most influential factors affecting house prices?</br>

üí° (I have previously worked on this competition in a group project, where the score was only about R¬≤ ‚âà 0.92, and the Kaggle Leaderboard performance was average. Some tasks such as EDA were completed by my teammates. This time, I want to complete it independently, optimize the process, and gain a deeper understanding of models like Random Forest and XGBoost, or possibly explore other better-performing models. By doing this, I can sharpen my skills within a limited timeframe, work more efficiently, and deliver a higher-quality side project.)</br>

## 3. Description of the Project
üìù Project Description</br>
This project aims to use the Kaggle House Prices dataset to build regression models for predicting house sale prices.
Independently conduct and optimize Exploratory Data Analysis (EDA) for deeper insights</br>
1. Experiment with different regression models (XGBoost, Random Forest, LightGBM, Lasso/Ridge Regression)</br>
2. Perform feature engineering (e.g., create HouseAge, RemodAge, TotalSF)</br>
3. Apply hyperparameter tuning to improve performance</br>
The final results will be submitted to the Kaggle Leaderboard, with comparisons and evaluations of different models.

## 4. Data Source(s)
Kaggle Competition : Housing Prices Competition for Kaggle Learn Users</br>
https://www.kaggle.com/competitions/home-data-for-ml-course

## 5. Tools & Technologies
1. Python (pandas, numpy, matplotlib, seaborn)</br>
2. scikit-learn (regression models, cross-validation, evaluation metrics)</br>
3. XGBoost / LightGBM</br>
4. Random Forest Regressor</br>
5. Jupyter Notebook</br>
6. GitHub (for version control and project documentation)

## 6. Planned Workflow / Methods
1. Data preprocessing (missing values, encoding, scaling)</br>
2. Exploratory Data Analysis (EDA) ‚Äì visualization and feature insights</br>
3. Feature engineering (create new variables such as HouseAge, RemodAge, TotalSF)</br>
4. Baseline models (Linear Regression, Random Forest, XGBoost)</br>
5. Model tuning (GridSearchCV / RandomizedSearchCV / Hyperparameter Tuning)</br>
6. Model comparison (R¬≤, RMSE, MAE)</br>
7. Submit the best result to the Kaggle Leaderboard</br>
8. Prepare a final report and presentation covering process, challenges, and results

## 7. Expected Outcome or Deliverables
üéØ Expected Deliverables
1. Improved performance and higher ranking on the Kaggle Leaderboard</br>
2. A complete data analysis and model comparison report</br>
3. A presentation covering research questions, EDA, feature engineering, model comparison, and results</br>
4. GitHub repository (including code, documentation, and Kaggle submissions)

